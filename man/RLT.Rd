% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RLT.r
\name{RLT}
\alias{RLT}
\title{\preformatted{            Reinforcement Learning Trees
}}
\usage{
RLT(
  x,
  y,
  censor = NULL,
  model = NULL,
  ntrees = if (reinforcement) 100 else 500,
  mtry = max(1, as.integer(ncol(x)/3)),
  nmin = max(1, as.integer(log(nrow(x)))),
  split.gen = "random",
  nsplit = 1,
  resample.replace = TRUE,
  resample.prob = if (resample.replace) 1 else 0.8,
  resample.preset = NULL,
  resample.track = FALSE,
  obs.w = NULL,
  var.w = NULL,
  linear.comb = 1,
  importance = FALSE,
  var.ready = FALSE,
  reinforcement = FALSE,
  param.control = list(),
  ncores = 0,
  verbose = 0,
  seed = NULL,
  ...
)
}
\arguments{
\item{x}{A \code{matrix} or \code{data.frame} of features}

\item{y}{Response variable. a \code{numeric}/\code{factor} vector.}

\item{censor}{The censoring indicator if survival model is used.}

\item{model}{The model type: \code{"regression"}, \code{"classification"}
or \code{"survival"}.}

\item{ntrees}{Number of trees, \code{ntrees = 100} if reinforcement is
used and \code{ntrees = 1000} otherwise.}

\item{mtry}{Number of randomly selected variables used at each
internal node.}

\item{nmin}{Terminal node size. Splitting will stop when the internal
node size is less than twice of \code{nmin}. This is
almost equivalent to setting \code{nodesize} \eqn{= 2 \times}
\code{nmin} in the \code{randomForest} package.}

\item{split.gen}{How the cutting points are generated: \code{"random"},
\code{"rank"} or \code{"best"}. If minimum child node size is
enforced (\code{alpha} $> 0$), then \code{"rank"} and \code{"best"}
should be used.}

\item{nsplit}{Number of random cutting points to compare for each
variable at an internal node.}

\item{resample.replace}{Whether the in-bag samples are obtained with
replacement.}

\item{resample.prob}{Proportion of in-bag samples.}

\item{resample.preset}{A pre-specified matrix for in-bag data indicator/count
matrix. It must be an \eqn{n \times} \code{ntrees}
matrix and cannot contain negative values. Extremely
large counts are not recommended, and the sum of
each column cannot exceed \eqn{n}. If provided, then
resample.track will be set to \code{TRUE}. This is an feature
is mainly use when estimating variances of a random forest.
Use at your own risk.}

\item{resample.track}{Whether to keep track of the observations used in each tree.}

\item{obs.w}{Observation weights}

\item{var.w}{Variable weights. If this is supplied, the default is to
perform weighted sampling of \code{mtry} variables. For
other usage, see the details of \code{split.rule} in
\code{\link{check_param_RLT}}.}

\item{linear.comb}{When \code{linear.comb} is larger than 1, a linear combination
split is used. When \code{reinforcement} is \code{TRUE}, the
variables with the highest potential at an internal node is used.
When \code{reinforcement} is \code{FALSE}, a marginal screening is
used. In both cases, SIR and SAVE are used to determine the coefficients
of the combination. When a categorical variable has the highest
potential, then a single variable is used. Currently restricted to
less than 5 number of variables in the linear combination.}

\item{importance}{Whether to calculate variable importance measures. The calculation
follows Breiman's original permutation strategy.}

\item{var.ready}{Construct \code{resample.preset} automatically to allow variance
estimations for prediction. If this is used, then \code{resample.replace}
will be set to \code{FALSE} and \code{resample.prob} should be no
larger than \eqn{n / 2}. It is recommended to use a very large
\code{ntrees}, e.g, 10000 or larger. For \code{resample.prob} greater than
\eqn{n / 2}, one should use the \code{\link{Reg_Var_Forest}} function.}

\item{reinforcement}{Should reinforcement splitting rule be used. Default
is \code{"FALSE"}, i.e., regular random forests. When it
is activated, embedded model tuning parameters are
automatically chosen. They can also be specified in
\code{RLT.control}.}

\item{param.control}{A list of additional parameters. This can be used to
specify other features in a random forest and set embedded
model parameters for reinforcement splitting rules.
See \code{check_param_RLT} and \code{set_embed_param} for
more details. using \code{reinforcement = TRUE} will automatically
generate some default tunings. However, they are not necessarily
good.}

\item{ncores}{Number of cores. Default is 0 (using all available cores).}

\item{verbose}{Whether fitting info should be printed.}

\item{seed}{Random seed number to replicate a previously fitted forest.
Internally, the \verb{xoshiro256++} generator is used. If not specified,
a seed will be generated automatically.}

\item{...}{Additional arguments.}
}
\value{
A \code{RLT} object, constructed as a list consisting

\item{FittedForest}{Fitted tree structures}
\item{VarImp}{Variable importance measures, if \code{importance = TRUE}}
\item{Prediction}{In-bag prediction values}
\item{OOBPrediction}{Out-of-bag prediction values}
\item{ObsTrack}{An \eqn{n \times} \code{ntrees} matrix that indicates which observations
are used in each tree. Provided if \code{resample.preset}
is used or \code{resample.track = TRUE}.}
}
\description{
\preformatted{      Fit models for regression, classification and 
                   survival analysis using reinforced splitting rules.
                   The model reduces to regular random forests if 
                   reinforcement is turned off.
}

If \code{x} is a data.frame, then all factors are treated as categorical variables.

To specify parameters of embedded models when \code{reinforcement = TRUE},
users can supply the following in the \code{param.control} list:

\itemize{\item \code{embed.ntrees}: number of trees in the embedded model
\item \code{embed.resample.prob}: proportion of samples (of the internal node)
in the embedded model \item \code{embed.mtry}: number or proportion of variables
\item \code{embed.nmin} terminal node size \item \code{embed.split.gen} random
cutting point search method (\code{"random"}, \code{"rank"} or \code{"best"}) \item \code{embed.nsplit}
number of random cutting points.}

For some other experimental features, please see \code{\link{check_param_RLT}}.
}
\references{
Zhu, R., Zeng, D., & Kosorok, M. R. (2015) "Reinforcement Learning Trees." Journal of the American Statistical Association. 110(512), 1770-1784.

\dontrun{}
}
